{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 16:46:54.826970: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-18 16:46:54.869940: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-18 16:46:54.870008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-18 16:46:54.871230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-18 16:46:54.877039: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-18 16:46:54.877423: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-18 16:46:55.922452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import bigrams, ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = \"../datasets\"\n",
    "file_name = \"tupi_binary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(file_name):\n",
    "    file_path = os.path.join(datasets_folder, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        return pd.read_csv(file_path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('portuguese'))  # Use the appropriate language for your text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and not any(char in word for char in ['https', '@'])]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    df['preprocessed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>researcher</th>\n",
       "      <th>year</th>\n",
       "      <th>aggressive</th>\n",
       "      <th>hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65848623693028e+18</td>\n",
       "      <td>@user @user @user quanto vc pagava na época da...</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, user, user, quanto, vc, pagava, época, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65848623777333e+18</td>\n",
       "      <td>@user os árabes já vão lhes chutar do país ??</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, árabes, vão, chutar, país, ?, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65848960585394e+18</td>\n",
       "      <td>@user @user @user @user @user tem que desenhar...</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, user, user, user, user, desenhar, expli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65849012716374e+18</td>\n",
       "      <td>@user @user chola mais gado. e se não quiser p...</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[user, user, chola, gado, ., quiser, pagar, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65849018793945e+18</td>\n",
       "      <td>michele micheque nao tinha cartao do bolsonaro...</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[michele, micheque, nao, cartao, bolsonaro, pq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source                    id  \\\n",
       "0  twitter  1.65848623693028e+18   \n",
       "1  twitter  1.65848623777333e+18   \n",
       "2  twitter  1.65848960585394e+18   \n",
       "3  twitter  1.65849012716374e+18   \n",
       "4  twitter  1.65849018793945e+18   \n",
       "\n",
       "                                                text      researcher  year  \\\n",
       "0  @user @user @user quanto vc pagava na época da...  oliveira et al  2023   \n",
       "1      @user os árabes já vão lhes chutar do país ??  oliveira et al  2023   \n",
       "2  @user @user @user @user @user tem que desenhar...  oliveira et al  2023   \n",
       "3  @user @user chola mais gado. e se não quiser p...  oliveira et al  2023   \n",
       "4  michele micheque nao tinha cartao do bolsonaro...  oliveira et al  2023   \n",
       "\n",
       "   aggressive  hate                                  preprocessed_text  \n",
       "0           1     1  [user, user, user, quanto, vc, pagava, época, ...  \n",
       "1           1     1            [user, árabes, vão, chutar, país, ?, ?]  \n",
       "2           1     1  [user, user, user, user, user, desenhar, expli...  \n",
       "3           1     1  [user, user, chola, gado, ., quiser, pagar, ba...  \n",
       "4           1     1  [michele, micheque, nao, cartao, bolsonaro, pq...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_terms = {'https', '@', 'user', 'link', '#', '??', '!!', '_:', '.:', '!:', '? ?', '! !', '_ :', '! :', '? :','mundo'}\n",
    "exclude_punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess a list of strings\n",
    "def preprocess_text(word_list):\n",
    "    # Remove exclusion terms\n",
    "    word_list = [word for word in word_list if word not in exclude_terms]\n",
    "\n",
    "    # Remove punctuation\n",
    "    word_list = [''.join(char for char in word if char not in exclude_punctuation) for word in word_list]\n",
    "\n",
    "    return word_list\n",
    "\n",
    "# Apply preprocessing to the 'preprocessed_text' column\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>researcher</th>\n",
       "      <th>year</th>\n",
       "      <th>aggressive</th>\n",
       "      <th>hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65848623693028e+18</td>\n",
       "      <td>@user @user @user quanto vc pagava na época da...</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[quanto, vc, pagava, época, , vc, diz, , , can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65848623777333e+18</td>\n",
       "      <td>@user os árabes já vão lhes chutar do país ??</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[árabes, vão, chutar, país, , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65848960585394e+18</td>\n",
       "      <td>@user @user @user @user @user tem que desenhar...</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[desenhar, explicar, desenho, pro, retardado, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65849012716374e+18</td>\n",
       "      <td>@user @user chola mais gado. e se não quiser p...</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[chola, gado, , quiser, pagar, barato, gasolin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitter</td>\n",
       "      <td>1.65849018793945e+18</td>\n",
       "      <td>michele micheque nao tinha cartao do bolsonaro...</td>\n",
       "      <td>oliveira et al</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[michele, micheque, nao, cartao, bolsonaro, pq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source                    id  \\\n",
       "0  twitter  1.65848623693028e+18   \n",
       "1  twitter  1.65848623777333e+18   \n",
       "2  twitter  1.65848960585394e+18   \n",
       "3  twitter  1.65849012716374e+18   \n",
       "4  twitter  1.65849018793945e+18   \n",
       "\n",
       "                                                text      researcher  year  \\\n",
       "0  @user @user @user quanto vc pagava na época da...  oliveira et al  2023   \n",
       "1      @user os árabes já vão lhes chutar do país ??  oliveira et al  2023   \n",
       "2  @user @user @user @user @user tem que desenhar...  oliveira et al  2023   \n",
       "3  @user @user chola mais gado. e se não quiser p...  oliveira et al  2023   \n",
       "4  michele micheque nao tinha cartao do bolsonaro...  oliveira et al  2023   \n",
       "\n",
       "   aggressive  hate                                  preprocessed_text  \n",
       "0           1     1  [quanto, vc, pagava, época, , vc, diz, , , can...  \n",
       "1           1     1                    [árabes, vão, chutar, país, , ]  \n",
       "2           1     1  [desenhar, explicar, desenho, pro, retardado, ...  \n",
       "3           1     1  [chola, gado, , quiser, pagar, barato, gasolin...  \n",
       "4           1     1  [michele, micheque, nao, cartao, bolsonaro, pq...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame with a 'text' column and a 'label' column\n",
    "texts = df['preprocessed_text'].tolist()\n",
    "labels = df['hate'].tolist()\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "y = np.array(labels)  # Assuming 'labels' is a list or array of integers\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Assuming y_train and y_test are lists or arrays of 0s and 1s\n",
    "# If not, you can use to_categorical to one-hot encode them\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_one_hot = y_train  # Assuming y_train is already a list or array of 0s and 1s\n",
    "y_test_one_hot = y_test  # Assuming y_test is already a list or array of 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "983/983 [==============================] - 215s 216ms/step - loss: 0.3697 - accuracy: 0.8807 - val_loss: 0.3641 - val_accuracy: 0.8815\n",
      "Epoch 2/10\n",
      "983/983 [==============================] - 223s 227ms/step - loss: 0.3662 - accuracy: 0.8808 - val_loss: 0.3640 - val_accuracy: 0.8815\n",
      "Epoch 3/10\n",
      "983/983 [==============================] - 222s 226ms/step - loss: 0.3659 - accuracy: 0.8808 - val_loss: 0.3639 - val_accuracy: 0.8815\n",
      "Epoch 4/10\n",
      "983/983 [==============================] - 221s 225ms/step - loss: 0.3659 - accuracy: 0.8808 - val_loss: 0.3647 - val_accuracy: 0.8815\n",
      "Epoch 5/10\n",
      "983/983 [==============================] - 205s 208ms/step - loss: 0.3660 - accuracy: 0.8808 - val_loss: 0.3639 - val_accuracy: 0.8815\n",
      "Epoch 6/10\n",
      "983/983 [==============================] - 178s 181ms/step - loss: 0.3658 - accuracy: 0.8808 - val_loss: 0.3649 - val_accuracy: 0.8815\n",
      "Epoch 7/10\n",
      "983/983 [==============================] - 182s 185ms/step - loss: 0.3659 - accuracy: 0.8808 - val_loss: 0.3673 - val_accuracy: 0.8815\n",
      "Epoch 8/10\n",
      "983/983 [==============================] - 203s 207ms/step - loss: 0.3660 - accuracy: 0.8808 - val_loss: 0.3639 - val_accuracy: 0.8815\n",
      "Epoch 9/10\n",
      "983/983 [==============================] - 191s 195ms/step - loss: 0.3659 - accuracy: 0.8808 - val_loss: 0.3643 - val_accuracy: 0.8815\n",
      "Epoch 10/10\n",
      "983/983 [==============================] - 185s 188ms/step - loss: 0.3658 - accuracy: 0.8808 - val_loss: 0.3646 - val_accuracy: 0.8815\n",
      "273/273 [==============================] - 10s 35ms/step - loss: 0.3780 - accuracy: 0.8752\n",
      "Test Accuracy: 0.8752003908157349\n"
     ]
    }
   ],
   "source": [
    "# Build the LSTM model\n",
    "embedding_dim = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # 1 neuron for binary classification with sigmoid activation\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train_one_hot, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tupi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
