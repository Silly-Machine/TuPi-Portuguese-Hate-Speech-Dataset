{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdeberta = pd.read_csv(\"result_tests/mdeberta/mDeBERTA-v3_str_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>researcher</th>\n",
       "      <th>year</th>\n",
       "      <th>aggressive</th>\n",
       "      <th>hate</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitter</td>\n",
       "      <td>-</td>\n",
       "      <td>Geraldo Alckmin vs Jair Bolsonaro A esquerda q...</td>\n",
       "      <td>fortuna et al</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitter</td>\n",
       "      <td>-</td>\n",
       "      <td>vacila não buceta mas eu faço o mesmo só sempr...</td>\n",
       "      <td>leite et al</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitter</td>\n",
       "      <td>-</td>\n",
       "      <td>um jovem passando vergonha na internet</td>\n",
       "      <td>leite et al</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitter</td>\n",
       "      <td>-</td>\n",
       "      <td>eu com o chip da claro 馃憥馃従</td>\n",
       "      <td>leite et al</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>instagram</td>\n",
       "      <td>-</td>\n",
       "      <td>o lixo veio a tona</td>\n",
       "      <td>vargas et al</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source id                                               text  \\\n",
       "0    twitter  -  Geraldo Alckmin vs Jair Bolsonaro A esquerda q...   \n",
       "1    twitter  -  vacila não buceta mas eu faço o mesmo só sempr...   \n",
       "2    twitter  -             um jovem passando vergonha na internet   \n",
       "3    twitter  -                        eu com o chip da claro 馃憥馃従   \n",
       "4  instagram  -                                 o lixo veio a tona   \n",
       "\n",
       "      researcher  year  aggressive  hate  prediction  probability  \n",
       "0  fortuna et al  2019           0     0         1.0     0.999965  \n",
       "1    leite et al  2020           1     0         1.0     0.999977  \n",
       "2    leite et al  2020           0     0         1.0     0.999972  \n",
       "3    leite et al  2020           0     0         0.0     0.000012  \n",
       "4   vargas et al  2021           0     0         0.0     0.000029  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mdeberta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2339.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mdeberta[\"prediction\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bertibau = pd.read_csv(\"BERTimbau_base_str_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4489.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bertibau[\"prediction\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bertibau2 = pd.read_csv(\"BERTimbau_large_str_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4575.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bertibau2[\"prediction\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5436226242271582\n",
      "Precision: 0.18447645667598367\n",
      "Recall: 0.8163653663177926\n",
      "F1 average: 0.5436226242271582\n",
      "ROC AUC Score: 0.726827229739612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_classification_metrics(df, predicted_col, label_col, probability_col, f1_average='binary'):\n",
    "    # Check if both classes are present in true labels\n",
    "    unique_labels = df[label_col].astype(int).unique()\n",
    "    if len(unique_labels) < 2:\n",
    "        raise ValueError(\"Both classes must be present in true labels for ROC AUC score.\")\n",
    "\n",
    "    # Calculate binary classification metrics\n",
    "    true_labels = df[label_col].astype(int)\n",
    "    predicted_labels = df[predicted_col].astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average=f1_average)\n",
    "\n",
    "    # Check if both classes are present in predicted probabilities\n",
    "    unique_probs = df[probability_col].unique()\n",
    "    if len(unique_probs) < 2:\n",
    "        raise ValueError(\"Both classes must be present in predicted probabilities for ROC AUC score.\")\n",
    "    \n",
    "    roc_auc = roc_auc_score(true_labels, df[probability_col])\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 average\": f1,\n",
    "        \"ROC AUC Score\": roc_auc\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "# Call the function with 'micro' averaging for F1 score\n",
    "metrics_result = evaluate_classification_metrics(df_mdeberta, predicted_col='prediction', label_col='hate', probability_col='probability', f1_average='micro')\n",
    "\n",
    "# Print the results\n",
    "for metric, value in metrics_result.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tupi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
